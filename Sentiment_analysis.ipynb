{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import CaboCha\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "import regex\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word(tree, chunk):\n",
    "    surface = ''\n",
    "    for i in range(chunk.token_pos, chunk.token_pos + chunk.token_size):\n",
    "        token = tree.token(i)\n",
    "        features = token.feature.split(',')\n",
    "        if features[0] == '名詞':\n",
    "            surface += token.surface\n",
    "        elif features[0] == '形容詞':\n",
    "            surface += features[6]\n",
    "            break\n",
    "        elif features[0] == '動詞':\n",
    "            surface += features[6]\n",
    "            break\n",
    "    return surface\n",
    "\n",
    "def get_2_words(line):\n",
    "    cp = CaboCha.Parser('-f1')\n",
    "    tree = cp.parse(line)\n",
    "    chunk_dic = {}\n",
    "    chunk_id = 0\n",
    "    for i in range(0, tree.size()):\n",
    "        token = tree.token(i)\n",
    "        if token.chunk:\n",
    "            chunk_dic[chunk_id] = token.chunk\n",
    "            chunk_id += 1\n",
    "\n",
    "    tuples = []\n",
    "    for chunk_id, chunk in chunk_dic.items():\n",
    "        if chunk.link > 0:\n",
    "            from_surface =  get_word(tree, chunk)\n",
    "            to_chunk = chunk_dic[chunk.link]\n",
    "            to_surface = get_word(tree, to_chunk)\n",
    "            tuples.append((from_surface, to_surface))\n",
    "    return tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#口コミを１文ずつに切り分けし、その文章内を係り受け解析し（単語A,単語B）にkawaii_wordがあればリストに加える\n",
    "def taples_kawaii(txt):\n",
    "    kawaii_word = ['可愛い','かわいい','カワイイ','美人','綺麗','きれい','美しい','美女']\n",
    "    #sen = regex.split(r'(?<=[。？])(?!$)', txt, flags=regex.VERSION1)\n",
    "    sen = re.split(r'(?<=[。？])(?!$)', txt)\n",
    "    add_list=[]\n",
    "    for i in sen:\n",
    "        tuples = get_2_words(i)\n",
    "        for j in tuples:\n",
    "            for k in kawaii_word:\n",
    "                if k in j:\n",
    "                    add_list.append(j)\n",
    "    return add_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#形態素解析するための関数を定義\n",
    "tagger = MeCab.Tagger('-Owakati')#タグはMeCab.Tagger（neologd辞書）を使用したい\n",
    "tagger.parse('')\n",
    "def tokenize_ja(text, lower):\n",
    "    node = tagger.parseToNode(str(text))\n",
    "    while node:\n",
    "        if lower and node.feature.split(',')[0] in [\"名詞\",\"形容詞\"]:#分かち書きで取得する品詞を指定\n",
    "            yield node.surface.lower()\n",
    "        node = node.next\n",
    "def tokenize(content, token_min_len, token_max_len, lower):\n",
    "    return [\n",
    "        str(token) for token in tokenize_ja(content, lower)\n",
    "        if token_min_len <= len(token) <= token_max_len and not token.startswith('_')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kawaii_wordが含まれた（単語A,単語B）をさらに形態素解析し、tennin_wordが含まれていたらフラグ立てをする\n",
    "def tennin_hantei(i):\n",
    "\n",
    "    txt = tokenize(i, 2, 10000, True)\n",
    "\n",
    "    vector_list =[]\n",
    "\n",
    "    a = 0\n",
    "\n",
    "    tennin_word = ['店員','スタッフ','女将','女性','店主','助手','奥さん','奥様','店長']\n",
    "    for k in txt:\n",
    "        if k in tennin_word:\n",
    "            a+=1\n",
    "        else:\n",
    "            a+=0        \n",
    "    if a >=1:\n",
    "        kawaii_flg = 1\n",
    "    else:\n",
    "        kawaii_flg = 0\n",
    "\n",
    "    return kawaii_flg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3943, 6)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('all_results.csv')\n",
    "df['store_score'].astype(float)\n",
    "df = df[df['store_score'] > 4.5]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                         | 0/3943 [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-f19e2bdb2198>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnew_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtxt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_sentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mnew_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtaples_kawaii\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mkawaii_flg_list2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mkawaii_flg_list3\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-66e2144991f7>\u001b[0m in \u001b[0;36mtaples_kawaii\u001b[1;34m(txt)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mkawaii_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'可愛い'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'かわいい'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'カワイイ'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'美人'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'綺麗'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'きれい'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'美しい'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'美女'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#sen = regex.split(r'(?<=[。？])(?!$)', txt, flags=regex.VERSION1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0msen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'(?<=[。？])(?!$)'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtxt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0madd_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msen\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\re.py\u001b[0m in \u001b[0;36msplit\u001b[1;34m(pattern, string, maxsplit, flags)\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[1;32mand\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mremainder\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mstring\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mreturned\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfinal\u001b[0m \u001b[0melement\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m     of the list.\"\"\"\n\u001b[1;32m--> 213\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxsplit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 400)\n",
    "list_sentence = df['dinner_review'].tolist()\n",
    "\n",
    "new_list=[]\n",
    "for txt in tqdm(list_sentence):\n",
    "    new_list.append(taples_kawaii(txt))\n",
    "kawaii_flg_list2=[]\n",
    "kawaii_flg_list3=[]\n",
    "for k in new_list:\n",
    "    kawaii_flg_list=[]\n",
    "    if len(k)==0:\n",
    "        kawaii_flg_list.append(0)\n",
    "    else: \n",
    "        for i in k:\n",
    "                kawaii_flg_list.append(tennin_hantei(i))\n",
    "                if tennin_hantei(i)==1:\n",
    "                    kawaii_flg_list3.append(sum(kawaii_flg_list))\n",
    "\n",
    "df['kawaii_flg']=kawaii_flg_list3\n",
    "df_ramen = df.groupby(['store_name','score'])['dinner_review'].apply(list).apply(' '.join).reset_index().sort_values('score', ascending=False)\n",
    "df_ramen2 = df.groupby(['store_name','score'])['kawaii_flg'].sum().reset_index().sort_values('kawaii_flg', ascending=False)\n",
    "df_ramen2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
