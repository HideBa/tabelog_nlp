{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from gensim.models import word2vec\n",
    "import MeCab\n",
    "import sys\n",
    "from collections import Counter\n",
    "import re\n",
    "import urllib.request, urllib.error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = MeCab.Tagger(\"-Owakati\")\n",
    "tagger.parse(\"\") # パーサーにデータを渡す前にこれを挟むことで、UnicodeDecodeErrorを避けることが出来る。\n",
    "                 #具体的な理由は分かっていないが、おそらく一度tagger.parse('')を挟むことで、プログラム内で使用されている標準の文字エンコードで初期化されるのではないかと思う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lunch_file = [\"pretest_sushitsuu_lunch.csv\",\n",
    "              \"pretest_sushiichi_lunch.csv\",\n",
    "              \"pretest_sawada_lunch.csv\",\n",
    "              \"pretest_mitani_lunch.csv\",\n",
    "              \"pretest_matsukan_lunch.csv\",\n",
    "              \"pretest_kiyoda_lunch.csv\",\n",
    "              \"pretest_imamura_lunch.csv\",\n",
    "              \"pretest_hatsune_lunch.csv\"\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dinner_file = [\"pretest_sushitsuu_dinner.csv\",\n",
    "              \"pretest_sushiichi_dinner.csv\",\n",
    "              \"pretest_sawada_dinner.csv\",\n",
    "              \"pretest_mitani_dinner.csv\",\n",
    "              \"pretest_matsukan_dinner.csv\",\n",
    "              \"pretest_kiyoda_dinner.csv\",\n",
    "              \"pretest_imamura_dinner.csv\",\n",
    "              \"pretest_hatsune_dinner.csv\",\n",
    "              \"pretest_aozora_dinner.csv\"\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_name = []\n",
    "for i in dinner_file:\n",
    "    df = pd.read_csv(i)\n",
    "    store_name.append(df.iloc[0][\"store_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt'\n",
    "slothlib_file = urllib.request.urlopen(url=url)\n",
    "stop_words = [line.decode(\"utf-8\").strip() for line in slothlib_file]\n",
    "stop_words = [sw for sw in stop_words if not sw==u'']\n",
    "my_stop_words = [\"℃/\", \"℃\", \"kg\", \"訪問\", \"寿司\", \"すし\"]\n",
    "stop_words += my_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_number(text):\n",
    "    # 連続した数字を0で置換\n",
    "    replaced_text = re.sub(r'\\d+', '0', text)\n",
    "    return replaced_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_ja(text, lower):\n",
    "    text = replaced_text = re.sub(r\"\\d+\", \"0\", text)\n",
    "    node = tagger.parseToNode(str(text))\n",
    "    while node:\n",
    "        surface = node.surface.lower()\n",
    "        if lower and node.feature.split(',')[0] in [\"名詞\",\"形容詞\"] and surface not in stop_words:\n",
    "            #分かち書きで取得する品詞を指定\n",
    "            yield surface\n",
    "            #lowerで小文字に変換 正規化\n",
    "        node = node.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(content, token_min_len, token_max_len, lower):\n",
    "    return [\n",
    "        str(token) for token in tokenize_ja(content, lower)\n",
    "        if token_min_len <= len(token) <= token_max_len and not token.startswith('_')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kyouki_lunch(path):\n",
    "    df = pd.read_csv(path)\n",
    "    wakati_sushi_text = []\n",
    "    word_list = []\n",
    "    many_words = []\n",
    "    similar_words = []\n",
    "    for i in df[\"lunch_review\"]:\n",
    "        txt = tokenize(i, 2, 10000, True)\n",
    "        wakati_sushi_text.append(txt)\n",
    "        for t in txt:\n",
    "            word_list.append(t)\n",
    "    #word2vec_sushi_model = word2vec.Word2Vec(word_list,sg=1,size=100, window=5,min_count=5,iter=100,workers=3)\n",
    "    counter = Counter(word_list)\n",
    "    for word, count in counter.most_common(10):\n",
    "        many_words.append(word)\n",
    "    print(\"昼間\" in word_list)\n",
    "    word2vec_sushi_model = word2vec.Word2Vec(wakati_sushi_text,sg=1,size=100, window=5,min_count=5,iter=100,workers=3)\n",
    "    for many in many_words:\n",
    "        #word2vec_sushi_model = word2vec.Word2Vec(wakati_sushi_text,sg=1,size=100, window=5,min_count=5,iter=100,workers=3)\n",
    "        similar = word2vec_sushi_model.most_similar(many)\n",
    "        similar_words.append(similar)\n",
    "    return many_words, similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kyouki_dinner(path):\n",
    "    df = pd.read_csv(path)\n",
    "    wakati_sushi_text = []\n",
    "    word_list = []\n",
    "    many_words = []\n",
    "    similar_words = []\n",
    "    for i in df[\"dinner_review\"]:\n",
    "        txt = tokenize(i, 2, 10000, True)\n",
    "        wakati_sushi_text.append(txt)\n",
    "        word_list.append(i)\n",
    "        for t in txt:\n",
    "            word_list.append(t)\n",
    "    counter = Counter(word_list)\n",
    "    for word, count in counter.most_common(10):\n",
    "        many_words.append(word)\n",
    "    word2vec_sushi_model = word2vec.Word2Vec(wakati_sushi_text,sg=1,size=100, window=5,min_count=5,iter=100,workers=3)\n",
    "    for many in many_words:\n",
    "        #word2vec_sushi_model = word2vec.Word2Vec(wakati_sushi_text,sg=1,size=100, window=5,min_count=5,iter=100,workers=3)\n",
    "        similar = word2vec_sushi_model.most_similar(many)\n",
    "        similar_words.append(similar)\n",
    "    return many_words, similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuki/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "True\n",
      "=====================\n",
      "True\n",
      "=====================\n",
      "True\n",
      "=====================\n",
      "True\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"word '昼間' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-01689165665a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msimilar_words_lunch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlunch_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmany_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilar_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkyouki_lunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmany_words_lunch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmany_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msimilar_words_lunch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilar_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-b0806a43a5ed>\u001b[0m in \u001b[0;36mkyouki_lunch\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmany\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmany_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m#word2vec_sushi_model = word2vec.Word2Vec(wakati_sushi_text,sg=1,size=100, window=5,min_count=5,iter=100,workers=3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0msimilar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec_sushi_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmany\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0msimilar_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmany_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilar_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1445\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1446\u001b[0m                 )\n\u001b[0;32m-> 1447\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1449\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m   1395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1396\u001b[0m         \"\"\"\n\u001b[0;32m-> 1397\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1399\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method will be removed in 4.0.0, use self.wv.wmdistance() instead\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word '昼間' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "many_words_lunch = []\n",
    "similar_words_lunch = []\n",
    "for path in lunch_file:\n",
    "    many_word, similar_word = kyouki_lunch(path)\n",
    "    many_words_lunch.append(many_word)\n",
    "    similar_words_lunch.append(similar_word)\n",
    "    print(\"=====================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfaffda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"lunch_review\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('pretest_sushitsuu_lunch.csv')\n",
    "#df_sushi = df.groupby(['store_name','score'])['review'].apply(list).apply(' '.join).reset_index().sort_values('score', ascending=False)\n",
    "#コーパス作成\n",
    "wakati_sushi_text = []\n",
    "word_list = []\n",
    "for i in df[\"lunch_review\"]:\n",
    "    txt = tokenize(i, 2, 10000, True)\n",
    "    wakati_sushi_text.append(txt)\n",
    "# モデル作成\n",
    "word2vec_sushi_model = word2vec.Word2Vec(wakati_sushi_text,sg=1,size=100, window=5,min_count=5,iter=100,workers=3)\n",
    "#sg（0: CBOW, 1: skip-gram）,size（ベクトルの次元数）,window（学習に使う前後の単語数）,min_count（n回未満登場する単語を破棄）,iter（トレーニング反復回数）\n",
    "#workers:複数のスレッドで処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_sushi_model.most_similar(\"握り\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'store_name': ['osaka', 'osaka', 'osaka', 'osaka', 'tokyo', 'tokyo', 'tokyo'],\n",
    "    'score': ['apple', 'orange', 'banana', 'banana', 'apple', 'apple', 'banana'],\n",
    "    'review': [\n",
    "        \"\"\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'city': ['osaka', 'osaka', 'osaka', 'osaka', 'tokyo', 'tokyo', 'tokyo'],\n",
    "    'food': ['apple', 'orange', 'banana', 'banana', 'apple', 'apple', 'banana'],\n",
    "    'price': [100, 200, 250, 300, 150, 200, 400],\n",
    "    'quantity': [1, 2, 3, 4, 5, 6, 7]\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"city\",\"price\"])[\"food\"].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"city\",\"price\"])[\"food\"].apply(list).apply(\" \".join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"city\",\"price\"])[\"food\"].apply(list).apply(\" \".join).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"city\",\"price\"])[\"food\"].apply(list).apply(\" \".join).reset_index().sort_values('price', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"city\",\"price\"])[\"food\"].apply(list).apply(\" \".join).reset_index().sort_values('price', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = [1,2,3,4,3,4,2,2]\n",
    "counter = Counter(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"握り\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
